{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Clustering players\nIn this tutorial we show how to cluster players to positions using their statistics. We will use a dataset provided by\nRonan Manning with statistics for everty top 5 leagues player, which is available [here](https://twitter.com/ronanmann/status/1408504415690969089?s=21).\nThis tutorial is inspired by John Muller's article on The Athletic [Introducing The Athletics 18 player roles](https://theathletic.com/3473297/2022/08/10/player-roles-the-athletic/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport warnings \nimport matplotlib.pyplot as plt\nfrom mplsoccer import Pitch\nimport os\nimport pathlib\nimport json\n\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nnp.random.seed(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the dataset\n\nWe open the data and change minutes played to integers. Then, we keep ony players who played more than 500 minutes.\nAs the next step, we convert all data to numeric format and replace NaN with 0. Last but not least, \nwe keep only columns\nwith player statistics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#open data\ndata = pd.read_csv(\"data.csv\")\n#change minutes to numerics \ndata[\"Min\"] = data[\"Min\"].apply(lambda x: x.replace(\",\", \"\")).astype(int)\n\ndata = data.loc[data[\"Min\"] > 500]\ndata = data.reset_index(drop = True)\ndata = data._convert(numeric=True)\ndata = data.fillna(0)\nX = data.iloc[:, 11:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality reduction\nSince we have over 100 features, it is important to reduce the dimensionality. First standard scaling \nis applied to data.\nThen, we use UMAP algorithm, as in John Muller's article, to reduce the dimensions to 2. As the next step we plot\nthe data with formation label available in the dataset. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\nfrom sklearn.preprocessing import StandardScaler\n#scale data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n#dim reduction\nreducer = UMAP(random_state = 2213)\ncomps = reducer.fit_transform(X)\n\n#plotting\nfig, ax = plt.subplots(figsize = (16,12))\n#map position to color\ncolors = {\"GK\": \"brown\", \"DF\": \"blue\", \"DF,MF\": \"aqua\", \"MF\": \"green\", \"MF,FW\": \"yellow\", \"FW,MF\": \"orange\", \"FW\": \"red\", 'DF,FW': \"purple\", 'FW,DF': \"pink\"}\ncolor_list = data.Pos.map(colors).to_list()\n\n#plot it\nfor i in range(X.shape[0]):\n    ax.scatter(comps[i,0], comps[i,1], c = color_list[i], s = 10)\nax.set_xlabel('U1')\nax.set_ylabel('U2')\n#make legend\nmarkers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in colors.values()]\nax.legend(markers, colors.keys(), numpoints=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering\nFrom the plot we claim that there are 6 clusters. We use hierarchical clustering (Ward method) to predict which\npoints belong to which cluster. Then, we plot the result of the clustering.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n#declare object\nscan = AgglomerativeClustering(n_clusters=6)\n#make predictions\nlabels = scan.fit_predict(comps)\nfig, ax = plt.subplots(figsize = (16,12))\nax.scatter(comps[:, 0], comps[:, 1], c=labels, s=10, cmap='gist_rainbow');\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labelling clusters\nTo label the clusters, we saved predcitins together with player names. Looking for many characteristic players,\nwe label them in 6 formations - goalkeepers, centre-backs, full-backs, defensive midfielders, attacking midfielders\nand strikers. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#explore the dataframe\nsee = data.iloc[:, 0:5]\nsee[\"label\"] = labels\nprint(see.sample(frac = 1).head(10))\n\n#plot predictions with a legend\nfig, ax = plt.subplots(figsize = (16,12))\nscatter = ax.scatter(comps[:, 0], comps[:, 1], c=labels, s=10, cmap='gist_rainbow');\nhandles = scatter.legend_elements()[0]\n#order the labels for hierarchy\nmyorder = [3,0,1,2,4,5]\nhandles = [handles[i] for i in myorder]\n#add legend\nax.legend(handles=handles, labels = [\"GK\", \"CB\", \"FB\", \"DM\", \"AM\", \"ST\"])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A different approach to the same problem - dimensionality reduction\nWe can also approach this problem using different techniques. First, we reduce the dimensions using a pipeline of PCA\nand tSNE. We also plot it the same way we did it for UMAP. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import Pipeline\n#declare dim reduction objects\npca = PCA()\ntsne = TSNE(random_state = 3454)\n#declare pipeline\nts = Pipeline([    ('pca', pca),\n    ('tsne', tsne)\n])\n#reduce dimensions\ncomps = ts.fit_transform(X)\n#plot it\nfig, ax = plt.subplots(figsize = (16,12))\nfor i in range(X.shape[0]):\n    ax.scatter(comps[i,0], comps[i,1], c = color_list[i], s = 10)\nax.set_xlabel('t1')\nax.set_ylabel('t2')\nmarkers = [plt.Line2D([0,0],[0,0],color=color, marker='o', linestyle='') for color in colors.values()]\nax.legend(markers, colors.keys(), numpoints=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A different approach to the same problem - clustering\nThis time we cluster the data using Gaussian Mixture Model clustering. It is the same model as in Muller's article. \nWe also plot the predictions on the 2 dimensional space. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n#declare object\ngmm = GaussianMixture(n_components=6, random_state=5).fit(comps)\n#make predictions\nlabels = gmm.predict(comps)\nfig, ax = plt.subplots(figsize = (16,12))\nplt.scatter(comps[:, 0], comps[:, 1], c=labels, s=10, cmap='gist_rainbow');\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A different approach to the same problem - labelling\nHere, we repeat steps we did previously to label the clusters. First, we investigate the dataframe with predictions,\nthen, we plot our clusters with labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "see = data.iloc[:, 0:5]\nsee[\"label\"] = labels\nprint(see.sample(frac = 1).head(10))\n\n#explore the dataframe\nfig, ax = plt.subplots(figsize = (16,12))\nscatter = ax.scatter(comps[:, 0], comps[:, 1], c=labels, s=10, cmap='gist_rainbow');\nhandles = scatter.legend_elements()[0]\n#order the labels for hierarchy\nmyorder = [4,2,5,1,0,3]\nhandles = [handles[i] for i in myorder]\nax.legend(handles=handles, labels = [\"GK\", \"CB\", \"FB\", \"DM\", \"AM\", \"ST\"])\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}