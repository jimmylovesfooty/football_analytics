{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xT\n",
    "\n",
    "Used python modules:\n",
    "```\n",
    "ipykernel\n",
    "mplsoccer\n",
    "statsmodels\n",
    "joblib\n",
    "scikit-learn\n",
    "xgboost\n",
    "XlsxWriter\n",
    "```\n",
    "\n",
    "The wyscoult data can be downloaded from\n",
    "- players https://figshare.com/ndownloader/files/15073721\n",
    "- events  https://figshare.com/ndownloader/files/14464685\n",
    "- matches https://figshare.com/ndownloader/files/14464622\n",
    "- teams   https://figshare.com/ndownloader/files/15073697\n",
    "\n",
    "Script assumes the data files the following structure:\n",
    "```  \n",
    "Wyscout\\events\\events_England.json\n",
    "Wyscout\\matches\\matches_England.json\n",
    "Wyscout\\players.json\n",
    "Wyscout\\teams.json\n",
    "xT.ipynb (this file)\n",
    "```\n",
    "Please update `data_root` etc if you have another layout.\n",
    "The script saves some debug file and the trained model to `output_folder`\n",
    "\n",
    "This first part loads all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesdavies\n"
     ]
    }
   ],
   "source": [
    "cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesdavies/soccermatics\n"
     ]
    }
   ],
   "source": [
    "cd soccermatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from 'data/Wyscout/events_France.json'\n",
      "Reading events from 'data/Wyscout/matches_France.json'\n",
      "Reading events from 'data/Wyscout/players.json'\n",
      "Reading events from 'data/Wyscout/teams.json'\n"
     ]
    }
   ],
   "source": [
    "# Imports and load events\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mplsoccer import Pitch, VerticalPitch\n",
    "\n",
    "# stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# standard libs\n",
    "import os, time, pathlib, warnings, itertools, json\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# Some naming conventions:\n",
    "# 'df_' is used for dataframes,\n",
    "# 's_' is used both for series and numpy 1D arrays even though they are different types\n",
    "# 'mask_' is used for boolean masks\n",
    "\n",
    "# Descriptions about the events/subevents can be found here:  https://dataglossary.wyscout.com/acceleration/\n",
    "# Tag ids, labels and descriptions can be found here https://support.wyscout.com/matches-wyid-events\n",
    "all_wyscout_tags = [\n",
    "    [101, \"goal\", \"Goal\"],\n",
    "    [102, \"own_goal\", \"Own goal\"],\n",
    "    [301, \"assist\", \"Assist\"],\n",
    "    [302, \"keypass\", \"Key pass\"],\n",
    "    [1901, \"counter_attack\", \"Counter attack\"],\n",
    "    [401, \"left\", \"Left foot\"],\n",
    "    [402, \"right\", \"Right foot\"],\n",
    "    [403, \"head_body\", \"Head/body\"],\n",
    "    [1101, \"direct\", \"Direct\"],\n",
    "    [1102, \"indirect\", \"Indirect\"],\n",
    "    [2001, \"dangerous_ball_lost\", \"Dangerous ball lost\"],\n",
    "    [2101, \"blocked\", \"Blocked\"],\n",
    "    [801, \"high\", \"High\"],\n",
    "    [802, \"low\", \"Low\"],\n",
    "    [1401, \"interception\", \"Interception\"],\n",
    "    [1501, \"clearance\", \"Clearance\"],\n",
    "    [201, \"opportunity\", \"Opportunity\"],\n",
    "    [1301, \"feint\", \"Feint\"],\n",
    "    [1302, \"missed ball\", \"Missed ball\"],\n",
    "    [501, \"free_space_r\", \"Free space right\"],\n",
    "    [502, \"free_space_l\", \"Free space left\"],\n",
    "    [503, \"take_on_l\", \"Take on left\"],\n",
    "    [504, \"take_on_r\", \"Take on right\"],\n",
    "    [1601, \"sliding_tackle\", \"Sliding tackle\"],\n",
    "    [601, \"anticipated\", \"Anticipated\"],\n",
    "    [602, \"anticipation\", \"Anticipation\"],\n",
    "    [1701, \"red_card\", \"Red card\"],\n",
    "    [1702, \"yellow_card\", \"Yellow card\"],\n",
    "    [1703, \"second_yellow_card\", \"Second yellow card\"],\n",
    "    [1201, \"gb\", \"Position: Goal low center\"],\n",
    "    [1202, \"gbr\", \"Position: Goal low right\"],\n",
    "    [1203, \"gc\", \"Position: Goal center\"],\n",
    "    [1204, \"gl\", \"Position: Goal center left\"],\n",
    "    [1205, \"glb\", \"Position: Goal low left\"],\n",
    "    [1206, \"gr\", \"Position: Goal center right\"],\n",
    "    [1207, \"gt\", \"Position: Goal high center\"],\n",
    "    [1208, \"gtl\", \"Position: Goal high left\"],\n",
    "    [1209, \"gtr\", \"Position: Goal high right\"],\n",
    "    [1210, \"obr\", \"Position: Out low right\"],\n",
    "    [1211, \"ol\", \"Position: Out center left\"],\n",
    "    [1212, \"olb\", \"Position: Out low left\"],\n",
    "    [1213, \"or\", \"Position: Out center right\"],\n",
    "    [1214, \"ot\", \"Position: Out high center\"],\n",
    "    [1215, \"otl\", \"Position: Out high left\"],\n",
    "    [1216, \"otr\", \"Position: Out high right\"],\n",
    "    [1217, \"pbr\", \"Position: Post low right\"],\n",
    "    [1218, \"pl\", \"Position: Post center left\"],\n",
    "    [1219, \"plb\", \"Position: Post low left\"],\n",
    "    [1220, \"pr\", \"Position: Post center right\"],\n",
    "    [1221, \"pt\", \"Position: Post high center\"],\n",
    "    [1222, \"ptl\", \"Position: Post high left\"],\n",
    "    [1223, \"ptr\", \"Position: Post high right\"],\n",
    "    [901, \"through\", \"Through\"],\n",
    "    [1001, \"fairplay\", \"Fairplay\"],\n",
    "    [701, \"lost\", \"Lost\"],\n",
    "    [702, \"neutral\", \"Neutral\"],\n",
    "    [703, \"won\", \"Won\"],\n",
    "    [1801, \"accurate\", \"Accurate\"],\n",
    "    [1802, \"not_accurate\", \"Not accurate\"],\n",
    "]\n",
    "\n",
    "\n",
    "def LoadEvents(file_name, data_folder):\n",
    "    # first check data folder\n",
    "    if os.path.exists(f\"{data_folder}/{file_name}\"):\n",
    "        file_path = pathlib.Path(f\"{data_folder}/{file_name}\")\n",
    "    # otherwise try current folder\n",
    "    elif os.path.exists(file_name):\n",
    "        file_path = pathlib.Path(file_name)\n",
    "    else:\n",
    "        warnings.warn(f\"File {file_name} not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Reading events from '{file_path}'\")\n",
    "    df = pd.read_json(file_path, encoding=\"unicode-escape\")\n",
    "    return df\n",
    "\n",
    "# Create folder where we can store all our generated files\n",
    "output_folder = \"outputs\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "current_country = \"France\"\n",
    "model_country = \"France\"\n",
    "data_root = \"Wyscout\"\n",
    "df_events = LoadEvents(f\"events_{current_country}.json\", f\"data/{data_root}\")\n",
    "df_matches = LoadEvents(f\"matches_{current_country}.json\", f\"data/{data_root}\")\n",
    "df_players = LoadEvents(\"players.json\", f\"data/{data_root}/\")\n",
    "df_teams = LoadEvents(\"teams.json\", f\"data/{data_root}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add labels to events\n",
    "\n",
    "Annotate the data frame with labels that might be good to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating events\n",
      "    Adding some spatial info\n",
      "    Adding tag masks\n",
      "    Adding Home/Away teams and goals\n",
      "    Adding player sequences\n",
      "    Adding xG\n",
      "3.76 seconds\n"
     ]
    }
   ],
   "source": [
    "def AnnotateEvents(df_in, pitch_dim):\n",
    "    df = df_in.copy()\n",
    "\n",
    "    print(f\"Annotating events\")\n",
    "\n",
    "    # Annotate with custom columns since it simplifies the analysis\n",
    "    # All custom columns starts with capital letter to avoid confusion with the original data\n",
    "\n",
    "    # Alternative eventsec that doesn't reset when there is a new match period\n",
    "    df[\"EventSec\"] = (\n",
    "        df.groupby([\"matchId\", \"matchPeriod\"]).tail(1)[\"eventSec\"].groupby(df.matchId).shift(1, fill_value=0).groupby(df.matchId).cumsum()\n",
    "    )\n",
    "    df[\"EventSec\"] = df.EventSec.bfill() + df.eventSec\n",
    "\n",
    "    print(f\"    Adding some spatial info\")\n",
    "    # Add start/end points and length for convenience\n",
    "\n",
    "    df[\"X0\"] = df.positions.apply(lambda x: x[0][\"x\"] * pitch_dim.length / 100)\n",
    "    df[\"Y0\"] = df.positions.apply(lambda x: x[0][\"y\"] * pitch_dim.width / 100)\n",
    "    df[\"X1\"] = df.positions.apply(lambda x: x[-1][\"x\"] * pitch_dim.length / 100)\n",
    "    df[\"Y1\"] = df.positions.apply(lambda x: x[-1][\"y\"] * pitch_dim.width / 100)\n",
    "    df[\"C0\"] = df.positions.apply(lambda x: abs(pitch_dim.width / 2 - x[0][\"y\"] * pitch_dim.width / 100))\n",
    "    df[\"C1\"] = df.positions.apply(lambda x: abs(pitch_dim.width / 2 - x[-1][\"y\"] * pitch_dim.width / 100))\n",
    "\n",
    "    df[\"X0_m1\"] = df.groupby(\"matchId\")[\"X0\"].shift(1, fill_value=0)\n",
    "    df[\"Y0_m1\"] = df.groupby(\"matchId\")[\"Y0\"].shift(1, fill_value=0)\n",
    "    df[\"X1_m1\"] = df.groupby(\"matchId\")[\"X1\"].shift(1, fill_value=0)\n",
    "    df[\"Y1_m1\"] = df.groupby(\"matchId\")[\"Y1\"].shift(1, fill_value=0)\n",
    "\n",
    "    # Patch save attempts so they have valid start points\n",
    "    df.loc[df.eventName == \"Save attempt\", \"X0\"] = df.X1\n",
    "    df.loc[df.eventName == \"Save attempt\", \"Y0\"] = df.Y1\n",
    "    # Patch shots end points to be at the goal center\n",
    "    df.loc[df.eventName == \"Shot\", \"X1\"] = pitch_dim.length\n",
    "    df.loc[df.eventName == \"Shot\", \"Y1\"] = pitch_dim.width / 2\n",
    "\n",
    "    df[\"ActionLength\"] = np.sqrt((df.X1 - df.X0) ** 2 + (df.Y1 - df.Y0) ** 2)\n",
    "    df[\"ActionAngle\"] = np.arctan2(df.Y1 - df.Y0, df.X1 - df.X0)\n",
    "    df[\"ActionDuration\"] = df.eventSec - df.eventSec.shift(-1, fill_value=0)\n",
    "\n",
    "    # Move x and y so they are relative to the goal center and store in local series\n",
    "    # hardcode penalty to be at 11,0\n",
    "    s_gx = np.where(df.subEventName == \"Penalty\", 11, pitch_dim.length - df.X0)\n",
    "    s_gy = np.where(df.subEventName == \"Penalty\", 0, pitch_dim.width / 2 - df.Y0)\n",
    "\n",
    "    # Use artctan2 to avoid division by zero\n",
    "    s_angle = np.arctan2(pitch_dim.goal_width * s_gx, s_gx**2 + s_gy**2 - (pitch_dim.goal_width / 2) ** 2)\n",
    "    df[\"Opp_goal_angle\"] = np.where(s_angle > 0, s_angle, s_angle + np.pi)\n",
    "    df[\"Opp_goal_dist\"] = np.sqrt(s_gx**2 + s_gy**2)\n",
    "    df[\"Own_goal_dist\"] = np.sqrt(df.X0**2 + (df.Y0 - pitch_dim.width / 2) ** 2)\n",
    "\n",
    "    # Add a bool column for each tag\n",
    "    print(f\"    Adding tag masks\")\n",
    "    df[\"Tags\"] = df.tags.apply(lambda x: [tag[\"id\"] for tag in x])\n",
    "    tags_to_use = [\"goal\", \"blocked\", \"keypass\", \"assist\", \"accurate\", \"not_accurate\", \"interception\", \"won\", \"lost\", \"neutral\", \"head_body\"]\n",
    "    for tag in all_wyscout_tags:\n",
    "        if tag[1] not in tags_to_use:\n",
    "            continue\n",
    "        tag_id = tag[0]\n",
    "        tag_str = f\"TAG_{tag[1]}\"\n",
    "        df[tag_str] = df.Tags.apply(lambda x: tag_id in x)\n",
    "\n",
    "    print(f\"    Adding Home/Away teams and goals\")\n",
    "    grouped_df = df.groupby(\"matchId\")\n",
    "    df[\"HomeTeamId\"] = grouped_df[\"teamId\"].transform(\"first\")\n",
    "    df[\"AwayTeamId\"] = (df.HomeTeamId != df.teamId).astype(int) * df.teamId\n",
    "    df[\"AwayTeamId\"] = grouped_df[\"AwayTeamId\"].transform(\"max\")\n",
    "    df[\"HomeTeamGoals\"] = (df[\"TAG_goal\"]) & (df[\"eventName\"] == \"Shot\") & (df.HomeTeamId == df.teamId)\n",
    "    df[\"AwayTeamGoals\"] = (df[\"TAG_goal\"]) & (df[\"eventName\"] == \"Shot\") & (df.AwayTeamId == df.teamId)\n",
    "    df[\"HomeTeamGoals\"] = grouped_df[\"HomeTeamGoals\"].cumsum()\n",
    "    df[\"AwayTeamGoals\"] = grouped_df[\"AwayTeamGoals\"].cumsum()\n",
    "\n",
    "    print(f\"    Adding player sequences\")\n",
    "    grouped_team_df = df[df.playerId != 0].groupby([\"matchId\", \"matchPeriod\", \"teamId\"])\n",
    "    df[\"PlayerSeqenceId\"] = grouped_team_df[\"playerId\"].transform(lambda x: np.add.accumulate(x != x.shift(-1, fill_value=0)).astype(int))\n",
    "    df[\"PlayerNextTeamPlayerId\"] = grouped_team_df[\"playerId\"].transform(lambda x: x.shift(-1, fill_value=0)).astype(int)\n",
    "    df[\"PlayerPrevTeamPlayerId\"] = grouped_team_df[\"playerId\"].transform(lambda x: x.shift(1, fill_value=0)).astype(int)\n",
    "\n",
    "    print(f\"    Adding xG\")\n",
    "\n",
    "    def CalcXG(df_sub):\n",
    "        if len(df_sub) == 0:\n",
    "            return 0\n",
    "        model = smf.glm(formula=\"TAG_goal ~ Opp_goal_dist + Opp_goal_angle\", data=df_sub, family=sm.families.Binomial()).fit()\n",
    "        b = model.params.values\n",
    "        s_xG = 1 / (1 + np.exp(b[0] + b[1] * df_sub.Opp_goal_dist + b[2] * df_sub.Opp_goal_angle))\n",
    "        return s_xG\n",
    "\n",
    "    mask_headers = (df.eventName == \"Shot\") & (df.TAG_head_body)\n",
    "    df.loc[mask_headers, \"XG\"] = CalcXG(df[mask_headers])\n",
    "\n",
    "    mask_non_headers = (df.eventName == \"Shot\") & (df.TAG_head_body == False)\n",
    "    df.loc[mask_non_headers, \"XG\"] = CalcXG(df[mask_non_headers])\n",
    "\n",
    "    mask_penalties = df[\"subEventName\"] == \"Penalty\"\n",
    "    df.loc[mask_penalties, \"XG\"] = CalcXG(df[mask_penalties])\n",
    "    df[\"XG\"] = df[\"XG\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Num matches to use, set to -1 to use all matches\n",
    "num_matches = -1\n",
    "matches = df_events.matchId.unique()[0:num_matches]\n",
    "df_events = df_events[df_events.matchId.isin(matches)]\n",
    "\n",
    "# create a pitch dim object for preprocessing but also for making sure all plots are the same\n",
    "pitch_dim = Pitch(pitch_type=\"custom\", pitch_length=105, pitch_width=68).dim\n",
    "start = time.time()\n",
    "df_annotated = AnnotateEvents(df_events, pitch_dim)\n",
    "print(f\"{time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create possession chains\n",
    "\n",
    "Modify `chain_ownership_rewards` and `chain_takeover_force` to create chains that fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75 seconds\n"
     ]
    }
   ],
   "source": [
    "def CreatePossessionChains(df_in):\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # We reward the teams a score depending on the actions in the table\n",
    "    # The rewards are accumulated and if they reach 4 points we switch ownership for the chain\n",
    "    chain_ownership_rewards = [\n",
    "        # [event team reward, other team reward, event, subevent, label (bool)]\n",
    "        #\n",
    "        # For these event we will always own the chain\n",
    "        [4, 0, \"Shot\", \"\", \"\"],\n",
    "        [4, 0, \"Free Kick\", \"\", \"\"],\n",
    "        [4, 0, \"Offside\", \"\", \"\"],\n",
    "        [4, 0, \"Pass\", \"\", \"TAG_accurate\"],\n",
    "        #\n",
    "        # We need two of these events in a row to own the chain\n",
    "        # [0, 2, \"Pass\", \"\", \"TAG_not_accurate\"],\n",
    "        [2, 0, \"Save attempt\", \"\", \"\"],\n",
    "        [2, 0, \"Others on the ball\", \"\", \"\"],\n",
    "        #\n",
    "        # Duels comes in pairs one for each team so we split them up, but we still want them in the same chain\n",
    "        [1, 0, \"Duel\", \"\", \"TAG_won\"],\n",
    "        [0, 1, \"Duel\", \"\", \"TAG_lost\"],\n",
    "        #\n",
    "        # Neutral duels are ignored\n",
    "        # [ 0, 0, \"Duel\", \"\", \"TAG_neutral\"],\n",
    "    ]\n",
    "\n",
    "    # We can force a change of ownership\n",
    "    chain_takeover_force = [\n",
    "        # [shift (1 = event after, 0= this event, -1= event before), event, subevent, label (bool)]\n",
    "        [1, \"Interruption\", \"\", \"\"],\n",
    "        [1, \"Foul\", \"\", \"\"],\n",
    "        #        [0, \"Free Kick\", \"\", \"\"],\n",
    "        [1, \"Save attempt\", \"\", \"TAG_goal\"],\n",
    "    ]\n",
    "\n",
    "    s_home = np.zeros(len(df))\n",
    "    s_away = np.zeros(len(df))\n",
    "    s_event_handled = np.zeros(len(df))\n",
    "    for c in chain_ownership_rewards:\n",
    "        mask = df.eventName == c[2]\n",
    "        if len(c[3]) > 0:\n",
    "            mask = mask & (df.subEventName == c[3])\n",
    "        if len(c[4]) > 0:\n",
    "            mask = mask & (df[c[4]])\n",
    "        mask = mask & (s_event_handled == 0)\n",
    "        mask_home = mask & (df.HomeTeamId == df.teamId)\n",
    "        mask_away = mask & (df.AwayTeamId == df.teamId)\n",
    "        s_home += mask_home * c[0] + mask_away * c[1]\n",
    "        s_away += mask_home * c[1] + mask_away * c[0]\n",
    "        s_event_handled = s_event_handled | mask\n",
    "\n",
    "    # Generate chain ids for home and away chains, then we group based on chain ids\n",
    "    s_home_chainId = np.add.accumulate(s_home.astype(bool) != s_home.astype(bool).shift(fill_value=0)).astype(int)\n",
    "    s_away_chainId = np.add.accumulate(s_away.astype(bool) != s_away.astype(bool).shift(fill_value=0)).astype(int)\n",
    "    # Group and sum the rewards per chain\n",
    "    s_home = s_home.groupby(s_home_chainId).transform(\"sum\")\n",
    "    s_away = s_away.groupby(s_away_chainId).transform(\"sum\")\n",
    "    # If a team reaches 4 points we claim ownership, 1 for home, and -1 for away\n",
    "    s_home = np.where(s_home >= 4, 1, 0)\n",
    "    s_away = np.where(s_away >= 4, -1, 0)\n",
    "    # Then we add them together and fill the gaps/zeros with the preceding value\n",
    "    combined = pd.Series(s_home + s_away).replace(0, np.nan).ffill()\n",
    "    # Then we assign the team ids\n",
    "    teams = pd.Series(np.where(combined == 1, df.HomeTeamId, df.AwayTeamId)).astype(int)\n",
    "    df[\"Possession_TeamId\"] = teams\n",
    "\n",
    "    # Now we geneate chains with 1 where ever we want to force chain start\n",
    "    # First where we want to force a chain\n",
    "    s_force = np.zeros(len(df))\n",
    "    for f in chain_takeover_force:\n",
    "        f_ = df.eventName == f[1]\n",
    "        if len(f[2]) > 0:\n",
    "            f_ = f_ & (df.subEventName == f[2])\n",
    "        if len(f[3]) > 0:\n",
    "            f_ = f_ & (df[f[3]])\n",
    "        s_force += f_.shift(f[0], fill_value=0)\n",
    "    # Then for each new half/period and whenever team ownership changes\n",
    "    s_newHalf = df.matchPeriod != df.matchPeriod.shift(fill_value=df.matchPeriod[0])\n",
    "    s_newTeam = teams != teams.shift(fill_value=teams[0])\n",
    "    # Add them and accumulate to get series that increases with 1 for every new chain start\n",
    "    df[\"Possession_Id\"] = np.add.accumulate(s_newTeam | s_newHalf | s_force.astype(bool)).astype(int)\n",
    "\n",
    "    # Annotate possession chains\n",
    "    # First track create new columns with the data we want to track\n",
    "    df[\"Possession_NumShots\"] = df.eventName == \"Shot\"\n",
    "    df[\"Possession_NumPasses\"] = (df.eventName == \"Pass\") & (df.teamId == df.Possession_TeamId) & (df.TAG_accurate)\n",
    "    df[\"Possession_HasGoal\"] = (df.eventName == \"Shot\") & (df.TAG_goal)\n",
    "\n",
    "    # Then group by possession chain and calculate the values\n",
    "    grouped_df = df.groupby(\"Possession_Id\")\n",
    "    df[\"Possession_NumShots\"] = grouped_df[\"Possession_NumShots\"].transform(\"sum\")\n",
    "    df[\"Possession_NumPasses\"] = grouped_df[\"Possession_NumPasses\"].transform(\"sum\")\n",
    "    df[\"Possession_HasGoal\"] = grouped_df[\"Possession_HasGoal\"].transform(\"any\")\n",
    "\n",
    "    # Then some we already had but not possession grouped\n",
    "    df[\"Possession_NumEvents\"] = grouped_df[\"Possession_Id\"].transform(\"count\")\n",
    "    df[\"Possession_xG\"] = grouped_df[\"XG\"].transform(\"max\")\n",
    "    df[\"Possession_ChainTime\"] = grouped_df[\"eventSec\"].transform(\"max\") - grouped_df[\"eventSec\"].transform(\"min\")\n",
    "    return df\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df_possession_chains = CreatePossessionChains(df_annotated)\n",
    "print(f\"{time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate labels to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.371209\n",
      "         Iterations 6\n",
      "                            Logit Regression Results                           \n",
      "===============================================================================\n",
      "Dep. Variable:     Possession_NumShots   No. Observations:               318317\n",
      "Model:                           Logit   Df Residuals:                   318313\n",
      "Method:                            MLE   Df Model:                            3\n",
      "Date:                 Wed, 09 Oct 2024   Pseudo R-squ.:                -0.06970\n",
      "Time:                         23:01:39   Log-Likelihood:            -1.1816e+05\n",
      "converged:                        True   LL-Null:                   -1.1046e+05\n",
      "Covariance Type:             nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X0             0.0030      0.000      8.230      0.000       0.002       0.004\n",
      "X1            -0.0089      0.000    -26.711      0.000      -0.010      -0.008\n",
      "C0            -0.0398      0.001    -68.030      0.000      -0.041      -0.039\n",
      "C1            -0.0560      0.001   -101.520      0.000      -0.057      -0.055\n",
      "==============================================================================\n",
      "          X0        X1        C0        C1\n",
      "X0  1.000000  0.722627  0.236520  0.025894\n",
      "X1  0.722627  1.000000  0.125692  0.093755\n",
      "C0  0.236520  0.125692  1.000000  0.261284\n",
      "C1  0.025894  0.093755  0.261284  1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ShowStatInfo(df, X_vars, y):\n",
    "    X = df[X_vars]\n",
    "    log_reg = sm.Logit(y, X).fit()\n",
    "    print(log_reg.summary())\n",
    "    matrix = X.corr()\n",
    "    print(matrix)\n",
    "    plt.imshow(matrix, cmap=\"Blues\")\n",
    "    plt.colorbar()\n",
    "    variables = []\n",
    "    for i in matrix.columns:\n",
    "        variables.append(i)\n",
    "    plt.xticks(range(len(matrix)), variables, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(matrix)), variables)\n",
    "    plt.title(\"Correlation matrix\")\n",
    "    plt.savefig(f\"{output_folder}/labels_correlation_matrix.svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df_to_train = df_possession_chains[df_possession_chains.eventName == \"Pass\"]\n",
    "X_vars = [\"X0\", \"X1\", \"C0\", \"C1\"]\n",
    "y_mask = df_to_train.Possession_NumShots > 0\n",
    "ShowStatInfo(df_to_train, X_vars, y_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train or load model then calculate predictions and xT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model and saving to outputs/France_shot_prob_model.pkl\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/__init__.py\", line 6, in <module>\n    from . import tracker  # noqa\n    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/tracker.py\", line 9, in <module>\n    from .core import _LIB, _check_call, make_jcargs\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/core.py\", line 269, in <module>\n    _LIB = _load_lib()\n           ^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/core.py\", line 222, in _load_lib\n    raise XGBoostError(\nxgboost.core.XGBoostError: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B9202094-7D52-318C-99CF-7034B0E9F28D> /opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/lib-dynload/../../libomp.dylib' (no such file), '/opt/homebrew/anaconda3/envs/data-ltv-prediction/bin/../lib/libomp.dylib' (no such file)\"]\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     89\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 90\u001b[0m df_trained \u001b[38;5;241m=\u001b[39m \u001b[43mCalculatePredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_to_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m df_trained[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShotProbability\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXG_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXT\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 73\u001b[0m, in \u001b[0;36mCalculatePredictions\u001b[0;34m(df_in, X_vars, y_mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing but required since model country \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_country\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from current country \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_country\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model and saving to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     model_A \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_XGB_C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# model = train_model_LR(X, y)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m#joblib.dump(model_A, model_filename)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model_A\u001b[38;5;241m.\u001b[39mpredict_proba(X)[::, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m, in \u001b[0;36mCalculatePredictions.<locals>.train_model_XGB_C\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     42\u001b[0m xgbC \u001b[38;5;241m=\u001b[39m xgboost\u001b[38;5;241m.\u001b[39mXGBClassifier(\n\u001b[1;32m     43\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m\n\u001b[1;32m     44\u001b[0m )  \u001b[38;5;66;03m# ,tree_method=\"hist\", device=\"cuda\")\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgbC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC AUC. Mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Std:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m xgbC\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    532\u001b[0m             some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    533\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m             )\n\u001b[1;32m    540\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(some_fits_failed_message, FitFailedWarning)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    544\u001b[0m     {\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HasMethods(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscoring\u001b[39m\u001b[38;5;124m\"\u001b[39m: [StrOptions(\u001b[38;5;28mset\u001b[39m(get_scorer_names())), \u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv_object\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Integral, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_dispatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Integral, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: [StrOptions({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m}), Real],\n\u001b[1;32m    557\u001b[0m     },\n\u001b[1;32m    558\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# estimator is not validated yet\u001b[39;00m\n\u001b[1;32m    559\u001b[0m )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_val_score\u001b[39m(\n\u001b[1;32m    561\u001b[0m     estimator,\n\u001b[0;32m--> 562\u001b[0m     X,\n\u001b[1;32m    563\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    565\u001b[0m     groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    570\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    571\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2*n_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m    574\u001b[0m ):\n\u001b[1;32m    575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate a score by cross-validation.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <cross_validation>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    [0.3315057  0.08022103 0.03531816]\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m     96\u001b[0m     {\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HasMethods(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    estimator : estimator object implementing 'fit'\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        The object to use to fit the data.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        The data to fit. Can be for example a list, or an array.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        The target variable to try to predict in the case of\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        supervised learning.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    groups : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        Group labels for the samples used while splitting the dataset into\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m        instance (e.g., :class:`GroupKFold`).\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 1.4\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m            ``groups`` can only be passed if metadata routing is not enabled\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m            via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m            is enabled, pass ``groups`` alongside other metadata via the ``params``\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m            argument instead. E.g.:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m            ``cross_validate(..., params={'groups': groups})``.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    scoring : str, callable, list, tuple, or dict, default=None\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m        Strategy to evaluate the performance of the cross-validated model on\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m        the test set.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m        If `scoring` represents a single score, one can use:\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        - a single string (see :ref:`scoring_parameter`);\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m        - a callable (see :ref:`scoring`) that returns a single value.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        If `scoring` represents multiple scores, one can use:\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m        - a list or tuple of unique strings;\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        - a callable returning a dictionary where the keys are the metric\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m          names and the values are the metric scores;\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m        - a dictionary with metric names as keys and callables a values.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m        See :ref:`multimetric_grid_search` for an example.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    cv : int, cross-validation generator or an iterable, default=None\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m        Determines the cross-validation splitting strategy.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        Possible inputs for cv are:\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m        - None, to use the default 5-fold cross validation,\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m        - int, to specify the number of folds in a `(Stratified)KFold`,\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m        - :term:`CV splitter`,\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        - An iterable yielding (train, test) splits as arrays of indices.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m        For int/None inputs, if the estimator is a classifier and ``y`` is\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        either binary or multiclass, :class:`StratifiedKFold` is used. In all\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        other cases, :class:`KFold` is used. These splitters are instantiated\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m        with `shuffle=False` so the splits will be the same across calls.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        Refer :ref:`User Guide <cross_validation>` for the various\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m        cross-validation strategies that can be used here.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m            ``cv`` default value if None changed from 3-fold to 5-fold.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        Number of jobs to run in parallel. Training the estimator and computing\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        the score are parallelized over the cross-validation splits.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        for more details.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    verbose : int, default=0\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m        The verbosity level.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    fit_params : dict, default=None\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the fit method of the estimator.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        .. deprecated:: 1.4\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m            This parameter is deprecated and will be removed in version 1.6. Use\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m            ``params`` instead.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    params : dict, default=None\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the underlying estimator's ``fit``, the scorer,\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m        and the CV splitter.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.4\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    pre_dispatch : int or str, default='2*n_jobs'\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m        Controls the number of jobs that get dispatched during parallel\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m        execution. Reducing this number can be useful to avoid an\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        explosion of memory consumption when more jobs get dispatched\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m        than CPUs can process. This parameter can be:\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m            - An int, giving the exact number of total jobs that are\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m              spawned\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m            - A str, giving an expression as a function of n_jobs,\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m              as in '2*n_jobs'\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    return_train_score : bool, default=False\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m        Whether to include train scores.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m        Computing training scores is used to get insights on how different\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m        parameter settings impact the overfitting/underfitting trade-off.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m        However computing the scores on the training set can be computationally\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        expensive and is not strictly required to select the parameters that\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m        yield the best generalization performance.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.21\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m            Default value was changed from ``True`` to ``False``\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    return_estimator : bool, default=False\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m        Whether to return the estimators fitted on each split.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    return_indices : bool, default=False\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m        Whether to return the train-test indices selected for each split.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    error_score : 'raise' or numeric, default=np.nan\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m        Value to assign to the score if an error occurs in estimator fitting.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m        If set to 'raise', the error is raised.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m        If a numeric value is given, FitFailedWarning is raised.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    scores : dict of float arrays of shape (n_splits,)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m        Array of scores of the estimator for each run of the cross validation.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m        A dict of arrays containing the score/time arrays for each scorer is\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m        returned. The possible keys for this ``dict`` are:\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m            ``test_score``\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m                The score array for test scores on each cv split.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m                Suffix ``_score`` in ``test_score`` changes to a specific\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m                metric like ``test_r2`` or ``test_auc`` if there are\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m                multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m            ``train_score``\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m                The score array for train scores on each cv split.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m                Suffix ``_score`` in ``train_score`` changes to a specific\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m                metric like ``train_r2`` or ``train_auc`` if there are\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m                multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m                This is available only if ``return_train_score`` parameter\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m                is ``True``.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            ``fit_time``\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m                The time for fitting the estimator on the train\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m                set for each cv split.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m            ``score_time``\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m                The time for scoring the estimator on the test set for each\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m                cv split. (Note time for scoring on the train set is not\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m                included even if ``return_train_score`` is set to ``True``\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m            ``estimator``\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m                The estimator objects for each cv split.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m                This is available only if ``return_estimator`` parameter\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m                is set to ``True``.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m            ``indices``\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m                The train/test positional indices for each cv split. A dictionary\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m                is returned where the keys are either `\"train\"` or `\"test\"`\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m                and the associated values are a list of integer-dtyped NumPy\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m                arrays with the indices. Available only if `return_indices=True`.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    cross_val_score : Run cross-validation for single metric evaluation.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    cross_val_predict : Get predictions from each split of cross-validation for\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m        diagnostic purposes.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m        loss function.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    >>> from sklearn import datasets, linear_model\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import cross_validate\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import make_scorer\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import confusion_matrix\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.svm import LinearSVC\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    >>> diabetes = datasets.load_diabetes()\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    >>> X = diabetes.data[:150]\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    >>> y = diabetes.target[:150]\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    >>> lasso = linear_model.Lasso()\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    Single metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    >>> cv_results = cross_validate(lasso, X, y, cv=3)\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    >>> sorted(cv_results.keys())\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    ['fit_time', 'score_time', 'test_score']\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    >>> cv_results['test_score']\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    array([0.3315057 , 0.08022103, 0.03531816])\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    Multiple metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    (please refer the ``scoring`` parameter doc for more information)\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    >>> scores = cross_validate(lasso, X, y, cv=3,\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    ...                         scoring=('r2', 'neg_mean_squared_error'),\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    ...                         return_train_score=True)\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    >>> print(scores['test_neg_mean_squared_error'])\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    [-3635.5... -3573.3... -6114.7...]\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    >>> print(scores['train_r2'])\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     params \u001b[38;5;241m=\u001b[39m _check_params_groups_deprecation(fit_params, params, groups)\n\u001b[1;32m    351\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     51\u001b[0m def __call__(self, iterable):\n\u001b[1;32m     52\u001b[0m     \"\"\"Dispatch the tasks and return the results.\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     Parameters\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         List of results of the tasks.\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[0;32m---> 65\u001b[0m     # Capture the thread-local scikit-learn configuration at the time\n\u001b[1;32m     66\u001b[0m     # Parallel.__call__ is issued since the tasks can be dispatched\n\u001b[1;32m     67\u001b[0m     # in a different thread depending on the backend and on the value of\n\u001b[1;32m     68\u001b[0m     # pre_dispatch and n_jobs.\n\u001b[1;32m     69\u001b[0m     config = get_config()\n\u001b[1;32m     70\u001b[0m     iterable_with_config = (\n\u001b[1;32m     71\u001b[0m         (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m         for delayed_func, args, kwargs in iterable\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1693\u001b[0m \n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/data-ltv-prediction/lib/python3.11/site-packages/joblib/parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "def CalculatePredictions(df_in, X_vars, y_mask):\n",
    "    # model variables\n",
    "    from itertools import combinations_with_replacement\n",
    "    import xgboost\n",
    "    import joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    df = df_in.copy()\n",
    "\n",
    "    def create_inputs(df_in, var):\n",
    "        inputs = []\n",
    "        # one variable combinations\n",
    "        for i, _ in enumerate(var):\n",
    "            inputs.extend(combinations_with_replacement(var, i))\n",
    "\n",
    "        # make new columns\n",
    "        df = df_in[var].copy()\n",
    "        columns = {}\n",
    "        for input in inputs:\n",
    "            if len(input) <= 1:\n",
    "                continue\n",
    "            column_name = \"\"\n",
    "            x = pd.Series(1, index=df.index)\n",
    "            for c in input:\n",
    "                column_name += c\n",
    "                x = x * df[c]\n",
    "            columns[column_name] = x\n",
    "            var.append(column_name)\n",
    "        df_dictionary = pd.DataFrame(columns)\n",
    "        df = pd.concat([df, df_dictionary], axis=1, ignore_index=False)\n",
    "\n",
    "        # print(df[var[:]].head(3).to_string())\n",
    "        return df, var\n",
    "\n",
    "    def train_model_XGB_C(X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123, stratify=y)\n",
    "        params = dict()\n",
    "        xgbC = xgboost.XGBClassifier(\n",
    "            n_estimators=100, ccp_alpha=0, max_depth=4, min_samples_leaf=10, random_state=123\n",
    "        )  # ,tree_method=\"hist\", device=\"cuda\")\n",
    "        scores = cross_val_score(estimator=xgbC, X=X_train, y=y_train, cv=10, n_jobs=-1, scoring=\"roc_auc\")\n",
    "        print(f\"ROC AUC. Mean: {np.mean(scores):.2f} Std:{np.std(scores):.2f}\")\n",
    "        xgbC.fit(X_train, y_train)\n",
    "        print(f\"Train score: {xgbC.score(X_train, y_train):.2f}\")\n",
    "        print(f\"Test score: {xgbC.score(X_test, y_test):.2f}\")\n",
    "        return xgbC\n",
    "\n",
    "    def train_model_LinReg(X, y):\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        return lr\n",
    "\n",
    "    df_inputs, var = create_inputs(df, X_vars)\n",
    "\n",
    "    X = df_inputs.values\n",
    "    y = y_mask.astype(int).values\n",
    "\n",
    "    model_filename = f\"{output_folder}/{model_country}_shot_prob_model.pkl\"\n",
    "    force_train = True\n",
    "    if os.path.exists(model_filename) and not force_train:\n",
    "        print(f\"Loading model from {model_filename}\")\n",
    "        model_A = joblib.load(model_filename)\n",
    "    else:\n",
    "        if model_country != current_country:\n",
    "            print(\n",
    "                f\"{model_filename} is missing but required since model country {model_country} is different from current country {current_country}.\"\n",
    "            )\n",
    "        print(f\"Training model and saving to {model_filename}\")\n",
    "        model_A = train_model_XGB_C(X, y)\n",
    "        # model = train_model_LR(X, y)\n",
    "        #joblib.dump(model_A, model_filename)\n",
    "    y_pred_proba = model_A.predict_proba(X)[::, 1]\n",
    "    df[\"ShotProbability\"] = y_pred_proba\n",
    "\n",
    "    # OLS\n",
    "    X2 = df_inputs[y_mask].values\n",
    "    y2 = df[y_mask].Possession_xG.values\n",
    "    model_B = train_model_LinReg(X2, y2)\n",
    "    y_pred = model_B.predict(X)\n",
    "    df[\"XG_pred\"] = y_pred\n",
    "    df[\"XT\"] = df[\"XG_pred\"] * df[\"ShotProbability\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df_trained = CalculatePredictions(df_to_train, X_vars, y_mask)\n",
    "print(f\"{time.time() - start:.2f} seconds\")\n",
    "df_trained[[\"ShotProbability\", \"XG_pred\", \"XT\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize for possesion and per 90 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlayerNormalizations(df_players_in, df_matches_in, df_teams, df_events):\n",
    "    df_players = df_players_in.copy()\n",
    "    df_matches = df_matches_in.copy()\n",
    "    matchIds = df_events.matchId.unique()\n",
    "    df_matches = df_matches.drop(df_matches[~df_matches.wyId.isin(matchIds)].index)\n",
    "    df_matches[\"MatchLength\"] = df_matches.wyId.map(df_events.groupby([\"matchId\"]).EventSec.max())\n",
    "\n",
    "    def ExtractTimes(x, df_events):\n",
    "        play_time = {}\n",
    "        matches_played = {}\n",
    "        possession_owner = {}\n",
    "        possession_total = {}\n",
    "        df_me = df_events[df_events.matchId == x.wyId]\n",
    "        for t in x.teamsData.values():\n",
    "            players_in = {}\n",
    "            players_out = {}\n",
    "            teamId = t[\"teamId\"]\n",
    "            lineup = t[\"formation\"][\"lineup\"]\n",
    "            for p in lineup:\n",
    "                players_in[p[\"playerId\"]] = 0\n",
    "                redCard = int(p[\"redCards\"]) * 60\n",
    "                if redCard >= 1:\n",
    "                    players_out[p[\"playerId\"]] = redCard\n",
    "            subs = t[\"formation\"][\"substitutions\"]\n",
    "            if subs != \"null\":\n",
    "                for s in subs:\n",
    "                    players_in[s[\"playerIn\"]] = int(s[\"minute\"]) * 60\n",
    "                    players_out[s[\"playerOut\"]] = int(s[\"minute\"]) * 60\n",
    "            for p in players_in:\n",
    "                if not p in players_out:\n",
    "                    players_out[p] = x.MatchLength\n",
    "                play_time[p] = players_out[p] - players_in[p]\n",
    "                df_playerEvents = df_me[(df_me.EventSec >= players_in[p]) & (df_me.EventSec <= players_out[p])]\n",
    "                df_teamTimes = (\n",
    "                    df_playerEvents.groupby([\"Possession_Id\"]).tail(1)[\"Possession_ChainTime\"].groupby([df_playerEvents.Possession_TeamId]).sum()\n",
    "                )\n",
    "                possession_owner[p] = df_teamTimes.get(teamId, 0)\n",
    "                possession_total[p] = df_teamTimes.sum()\n",
    "        return [play_time, possession_owner, possession_total]\n",
    "\n",
    "    times = df_matches.apply(lambda x: ExtractTimes(x, df_events), axis=1)\n",
    "\n",
    "    players = df_events.playerId.unique()\n",
    "    for p in players:\n",
    "        if p == 0:\n",
    "            continue\n",
    "        player_time = times.apply(lambda x: x[0].get(p, 0)).sum()\n",
    "        matches_played = times.apply(lambda x: 1 if p in x[0] else 0).sum()\n",
    "        possession_owner = times.apply(lambda x: x[1].get(p, 0)).sum()\n",
    "        possession_total = times.apply(lambda x: x[2].get(p, 0)).sum()\n",
    "        player_row = df_players.loc[df_players.wyId == p].iloc[0]\n",
    "        df_players.loc[df_players.wyId == p, \"TimePlayed\"] = (player_time / 60).astype(int)\n",
    "        df_players.loc[df_players.wyId == p, \"MatchesPlayed\"] = matches_played.astype(int)\n",
    "        df_players.loc[df_players.wyId == p, \"TotalChainTime\"] = possession_total.astype(int)\n",
    "        df_players.loc[df_players.wyId == p, \"OwnerChainTime\"] = possession_owner.astype(int)\n",
    "        df_players.loc[df_players.wyId == p, \"NormPer90\"] = (90 * 60) / player_time\n",
    "        df_players.loc[df_players.wyId == p, \"NormPossession\"] = possession_owner / possession_total\n",
    "\n",
    "    df_players[\"TeamName\"] = df_players.currentTeamId.map(df_teams.set_index(\"wyId\")[\"name\"]).fillna(\"Unknown\")\n",
    "    return df_players\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df_players = PlayerNormalizations(df_players, df_matches, df_teams, df_possession_chains)\n",
    "print(f\"{time.time() - start:.2f} seconds\")\n",
    "\n",
    "df_players[\"XT\"] = df_players.wyId.map(df_trained.groupby([\"playerId\"]).XT.sum())\n",
    "df_players[\"XT_normalized\"] = df_players[\"XT\"] / df_players[\"NormPossession\"] * df_players[\"NormPer90\"]\n",
    "vars = [\"shortName\", \"TeamName\", \"wyId\", \"XT_normalized\", \"TimePlayed\", \"XT\", \"NormPossession\", \"NormPer90\"]\n",
    "df_players.loc[df_players.TimePlayed > 400, vars].sort_values(by=[\"XT_normalized\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw one of the chains with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawChain(df_in, chain_id, pitch_dim, label):\n",
    "    mask_chain = (df_in.Possession_Id == chain_id) & (df_in.Possession_TeamId == df_in.teamId)\n",
    "    df_chain = df_in[mask_chain]\n",
    "    mask_label = df_chain[label].notna()\n",
    "    df_label = df_chain[mask_label]\n",
    "    max_value = df_label[label].max()\n",
    "    pitch = Pitch(line_color=\"black\", pitch_type=pitch_dim, line_zorder=2)\n",
    "    fig, ax = pitch.grid(grid_height=0.9, title_height=0.06, axis=False, endnote_height=0.04, title_space=0, endnote_space=0)\n",
    "    # add size adjusted arrows\n",
    "    for i, row in df_label.iterrows():\n",
    "        value = row[label]\n",
    "        # adjust the line width so that the more passes, the wider the line\n",
    "        line_width = value / max_value * 10\n",
    "        # get angle\n",
    "        # plot lines on the pitch\n",
    "        pitch.arrows(row.X0, row.Y0, row.X1, row.Y1, alpha=0.6, width=line_width, zorder=2, color=\"blue\", ax=ax[\"pitch\"])\n",
    "        # annotate text\n",
    "        ax[\"pitch\"].text(\n",
    "            (row.X0 + row.X1 - 8) / 2,\n",
    "            (row.Y0 + row.Y1 - 4) / 2,\n",
    "            str(value)[:5],\n",
    "            fontweight=\"bold\",\n",
    "            color=\"blue\",\n",
    "            zorder=4,\n",
    "            fontsize=20,\n",
    "            rotation=int((row.ActionAngle * 180 / np.pi + 90) % 180 - 90),\n",
    "        )\n",
    "\n",
    "    # shot\n",
    "    shot = df_chain.loc[df_chain.eventName == \"Shot\"]\n",
    "    pitch.arrows(shot.X0, shot.Y0, shot.X1, shot.Y1, width=line_width, color=\"red\", ax=ax[\"pitch\"], zorder=3)\n",
    "    # non lables like arrows\n",
    "    df_not_label = df_chain.loc[mask_label == False].iloc[:-1]\n",
    "    pitch.lines(df_not_label.X0, df_not_label.Y0, df_not_label.X1, df_not_label.Y1, color=\"grey\", lw=1.5, ls=\"dotted\", ax=ax[\"pitch\"])\n",
    "    ax[\"title\"].text(0.5, 0.5, f\"Actions leading to a shot with {label}\", ha=\"center\", va=\"center\", fontsize=30)\n",
    "    plt.savefig(f\"{output_folder}/chain_{chain_id}_{label}.svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df_possession_chains[\"XT\"] = df_trained[\"XT\"]\n",
    "df_possession_chains[\"ShotProbability\"] = df_trained[\"ShotProbability\"]\n",
    "df_possession_chains[\"XG_pred\"] = df_trained[\"XG_pred\"]\n",
    "DrawChain(df_possession_chains, 5, pitch_dim, \"XT\")\n",
    "DrawChain(df_possession_chains, 5, pitch_dim, \"ShotProbability\")\n",
    "DrawChain(df_possession_chains, 5, pitch_dim, \"XG_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump possesion chains to files\n",
    "\n",
    "One text file with some chain info.\n",
    "\n",
    "One excel file with the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DumpPossessionChains(filename, df):\n",
    "    with open(filename, \"w\") as f:\n",
    "        print(f\"Dumping possession chains to '{filename}'\")\n",
    "        with redirect_stdout(f):\n",
    "\n",
    "            def PrintHelp_TagDescs(event):\n",
    "                r = []\n",
    "                for tagId in event.tags:\n",
    "                    r.append([tag[2] for tag in all_wyscout_tags if tag[0] == tagId[\"id\"]][0])\n",
    "                return sorted(r)\n",
    "\n",
    "            def PrintHelp_Time(event):\n",
    "                m, s = divmod(int(event.eventSec), 60)\n",
    "                h, m = divmod(m, 60)\n",
    "                return f\"{m:02.0f}:{s:02.0f} ({event.matchPeriod})\"\n",
    "\n",
    "            lastId = -1\n",
    "            for i, event in df.iterrows():\n",
    "                if event.Possession_Id != lastId:\n",
    "                    print(f\"--- Chain: {event.Possession_Id} Team: {event.Possession_TeamId} ---\")\n",
    "                print(\n",
    "                    f\"[{i:0>{3}}]\\t [{PrintHelp_Time(event)}]\\t {event.teamId} '{event.eventName}' - '{event.subEventName}' - {PrintHelp_TagDescs(event)}\"\n",
    "                )\n",
    "                lastId = event.Possession_Id\n",
    "\n",
    "\n",
    "max_num_output_rows = 10000\n",
    "df_to_dump = df_possession_chains[:max_num_output_rows]\n",
    "#DumpPossessionChains(f\"{output_folder}/{current_country}_possession_chain.txt\", df_to_dump)\n",
    "\n",
    "#out_excel_file_name = f\"{output_folder}/{current_country}_possession_chains.xlsx\"\n",
    "#with pd.ExcelWriter(out_excel_file_name) as writer:\n",
    "#    print(f\"Dumping data frame in excel format to '{out_excel_file_name}'\")\n",
    " #   columnsToDrop = [\"positions\", \"eventId\", \"subEventId\", \"id\", \"matchId\"]\n",
    "#   df_to_dump.drop(columns=columnsToDrop).to_excel(writer, sheet_name=\"Alternative\", float_format=\"%.2f\", freeze_panes=(1, 1))\n",
    " #   for worksheet in writer.sheets.values():\n",
    " #       worksheet.autofit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a summary of the events\n",
    "\n",
    "Useful to get a quick overview of the events in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DumpSummaryToFile(file_name):\n",
    "    df_all_events = LoadEvents(file_name, \"Wyscout/events/\")\n",
    "    out_file_name = f\"{output_folder}/{file_name[:-len('.json')]}_summary.txt\"\n",
    "    with open(out_file_name, \"w\") as f:\n",
    "        print(f\"Dumping summary to '{out_file_name}'\")\n",
    "        with redirect_stdout(f):\n",
    "            print(f\"Total number of events in '{file_name}': {len(df_all_events)}\")\n",
    "            # Print the number of events, subevents and tags to get a feeling for the data\n",
    "            # A bit slow but it's only run once\n",
    "            print(\"Number of events, subevents and tags:\")\n",
    "            events = sorted(df_all_events.eventName.unique())\n",
    "            for event in events:\n",
    "                df_event = df_all_events[df_all_events.eventName == event]\n",
    "                print(\"\\n-------------------------------------------------------------\")\n",
    "                print(f\"'{event}': {len(df_event)} ({100*len(df_event)/len(df_all_events):.2f}% of all events)\")\n",
    "                sub_events = sorted(df_event.subEventName.unique())\n",
    "                for sub_event in sub_events:\n",
    "                    df_sub_event = df_event[df_event.subEventName == sub_event]\n",
    "                    print(f\"    '{sub_event}': {len(df_sub_event)} ({100*len(df_sub_event)/len(df_event):.2f}% of '{event}' events)\")\n",
    "                    for tag in all_wyscout_tags:\n",
    "                        tag_id = tag[0]\n",
    "                        mask_tag = df_sub_event.tags.apply(lambda x: tag_id in [tag[\"id\"] for tag in x])\n",
    "                        num_tags = len(df_sub_event[mask_tag])\n",
    "                        if num_tags > 0:\n",
    "                            # Note that an event can have multiple tags or none\n",
    "                            print(f\"        '{tag[2]}': {num_tags} ({100*num_tags/len(df_sub_event):.2f}%)\")\n",
    "\n",
    "\n",
    "DumpSummaryToFile(f\"events_{current_country}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
